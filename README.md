# ğŸš€ End-to-End ELT Data Pipeline

This repository contains a robust, production-ready **ELT (Extract, Load, Transform)** pipeline built using **Python**, **Docker**, and **dbt**. The pipeline automates the extraction of data from a source database, loads it into a destination warehouse, and transforms it using dbt for analytical purposes.

---

## ğŸ“Œ Features

- ğŸ”„ **Extract**: Connects to the source database and pulls data efficiently.
- ğŸ“¥ **Load**: Loads raw data into the destination database.
- ğŸ› ï¸ **Transform**: Leverages **dbt** to apply business logic and prepare clean, analytics-ready data.
- ğŸ³ **Dockerized**: Run the entire pipeline in isolated containers for consistent execution.
- ğŸ•’ **Scheduled Automation**: Easily schedule runs using **cron jobs**.

---

## ğŸ§© Architecture Workflow

```text
   [Source Database] 
         â¬‡
      Extraction
         â¬‡
  [Raw Layer in Destination DB]
         â¬‡
     Transformation (dbt)
         â¬‡
  [Cleaned & Modeled Data]
```


## ğŸ› ï¸ Technologies Used

| Component     | Tool / Framework         |
| ------------- | ------------------------ |
| Language      | Python                   |
| Orchestration | Subprocess + Cron Jobs   |
| Data Modeling | dbt                      |
| Containers    | Docker + Docker Compose  |
| Database      | PostgreSQL (replaceable) |


## ğŸš€ Getting Started

âœ… Prerequisites

``` bash
   Install Docker

   Install Python and pip

   Install dbt:
```
```bash 
  pip install dbt-core dbt-postgres --user
```

## ğŸ“¦ Setup & Run

``` bash
## 1. Clone the Repository:

git clone https://github.com/Shubham11440/ELT-pipeline.git
cd ELT-pipeline

## 2. Configure Database Connections:

Update credentials inside the config/ directory.

Example config files:

      config/source_db_config.yaml

      config/destination_db_config.yaml

## 3. Set Up dbt:

      Go to the elt/dbt_project/ directory.

Update profiles.yml for your DB connection.

## 4. Build Docker Containers:

     docker-compose up --build

## 5. Run the Pipeline:

Manually:

   python run_pipeline.py

Or schedule it using a cron job:

      0 2 * * * /usr/bin/python3 /path/to/ELT-pipeline/run_pipeline.py

## 6. Stop & Clean Up:

      docker-compose down -v

```

## ğŸ“ Project Structure

```
ELT-pipeline/
â”‚
â”œâ”€â”€ config/               # YAML configs for DB connections
â”œâ”€â”€ elt/                  # Core pipeline scripts
â”œâ”€â”€ custom_postgres/      # Optional PostgreSQL setup
â”œâ”€â”€ logs/                 # Logging directory
â”œâ”€â”€ source_db_init/       # SQL seed scripts for source DB
â”œâ”€â”€ docker-compose.yaml   # Docker setup
â”œâ”€â”€ start.sh              # Startup script
â””â”€â”€ README.md             # Project documentation
```

## ğŸ“˜ Documentation

ğŸ”§ Configuration Guide â†’ docs/configuration.md

ğŸ“Š dbt Setup & Usage â†’ docs/dbt.md

ğŸ³ Docker Instructions â†’ docs/docker.md

